---
title: "Thoughts on data pipelines"
date: 2025-10-03
summary: "Since I most likely will shift my focus away from data pipelnes I try to summarize what I've learned"
authors: 
    - qnde
category:
    - data engineering
---

Getting the data to the place where it is consumed is still a challenge - it seems even more than the challenging than development of the actual AI algorithms which end up in your product.
In our project, we went through several stages of tackling this problem and I want to sketch a few of those problems here.
<!-- more -->

### 1. No end-to-end responsibility for data pipelines

Fixing data pipelines is really hard when you first have to figure out who can help you with your missing data.
In a large project, you can spend a substantial amount of time where you get referred to many different teams until you find the right person that can help you with your issue.
This occurs when the data piplelines become complex and stretch accross multiple teams among which no team has end-to-end responsibility.
The lack of end-to-end responsibility directly leads to a lack of observability because no team would care about monitoring the full data pipeline.

This issue can be fixed by establishing stream aligned teams which have this responsibility. 
It is really important though that those teams have both the technical means as well as organizatorial to take on this responsibility.
For example, if you separate your data engineers from the data scientists which consume the data in different teams this is impossible.

### 2. Centralized data platforms are bound to fail

"The central data platform for everyone in the project" always poses a bottleneck for the data consumers besides the obvious synergies you gain from not having all teams having to set up their own plattform.

The reason is first of all that users which are forced to use a central plattform most likely will not use it in a sensible way.
Each of the stream aligned teams should at least contain one platform expert who knows how to use the platform properly. 
If this is not the case the 'noisy neighbour' problem will eventually degrade the performance for all data pipelines. 

It is much better to have a de-centralized/federated data platform where each team can use the tools they are familiar with.
The data itself can reside in a common data lake (with governance etc.) such that complexity is only introduced on the "compute side of things" but not on the data.  

### 3. Costs for processing data scale linearly but expected revenue does not

In a project that develops a new product it's hard to estimate the revenue it generates.
The best proxy for that I can give is simply the performance KPI of the machine learning algorithms which consume the incoming data.
These performance KPI **do not scale linear with respect to the amount of data used for training**.
This is a problem as the costs will always scale linearly with the amount of data as you have to pay for additional compute, storage, license costs etc..
There are two ways to tackle the problem:

1. Try to reduce costs by switching to cheaper solutions for the bulk data and therby "flatting" the cost curve
2. Try to only process data from which the KPI would improve significantly 

Both of those approaches have their own issues. 
The main problem of the first one is that you are basically in a constant migration process to make the processing cheaper and cheaper.
This introduces a significant development overhead for the teams developing the data pipelines.
The second approach is difficult as you have to have solved the problem which your AI algorithm wants to approximate "in some other way".
This leads to a classic bootstrapping problem.
For example, if you would have an AI which is capable of deciding which data is important for the training of another AI: Why not use this AI to solve the original problem?

At this point it is still unclear to me which of the two approaches is the more promising one.

