---
title: "Thoughts on data pipelines"
date: 2025-10-03
summary: "Since I most likely will shift my focus away from data pipelnes I try to summarize what I've learned"
authors: 
    - qnde
category:
    - data engineering
---

Getting the data to the place where it is consumed is still a challenge - it seems even more than the challenging than development of the actual AI algorithms which end up in your product.
In our project, we went through several stages of tackling this problem and I want to sketch a few of those problems here.
<!-- more -->

### 1. No end-to-end responsibility for data pipelines

Fixing data pipelines is really hard when you first have to figure out who can help you with your missing data.
In a large project, you can spend a substantial amount of time where you get referred to many different teams until you find the right person that can help you with your issue.
This occurs when the data piplelines become complex and stretch accross multiple teams among which no team has end-to-end responsibility.
The lack of end-to-end responsibility directly leads to a lack of observability because no team would care about monitoring the full data pipeline.

This issue can be fixed by establishing stream aligned teams which have this responsibility. 
It is really important though that those teams have both the technical means as well as organizatorial to take on this responsibility.
For example, if you separate your data engineers from the data scientists which consume the data in different teams this is impossible.

### 2. Centralized data platforms are bound to fail

"The central data platform for everyone in the project" always poses a bottleneck for the data consumers besides the obvious synergies you gain from not having all teams having to set up their own plattform.

The reason is first of all that users which are forced to use a central plattform most likely will not use it in a sensible way.
Each of the stream aligned teams should at least contain one platform expert who knows how to use the platform properly. 
If this is not the case the 'noisy neighbour' problem will eventually degrade the performance for all data pipelines. 

It is much better to have a de-centralized/federated data platform where each team can use the tools they are familiar with.
The data itself can reside in a common data lake (with governance etc.) such that complexity is only introduced on the "compute side of things" but not on the data.  

### 3. Costs for processing data scale linearly but expected revenue does not

In a project that develops a new product it's hard to estimate the revenue it generates.
The best proxy for that I can give is simply the performance KPI of the machine learning algorithms which consume the incoming data.
These performance KPI **do not scale linear with respect to the amount of data used for training**.
This is a problem as the costs will always scale linearly with the amount of data as you have to pay for additional compute, storage, license costs etc..
There are two ways to tackle the problem:

1. Try to reduce costs by switching to cheaper solutions for the bulk data and therby "flatting" the cost curve
2. Try to only process data from which the KPI would improve significantly 

Both of those approaches have their own issues. 
The main problem of the first one is that you are basically in a constant migration process to make the processing cheaper and cheaper.
This introduces a significant development overhead for the teams developing the data pipelines.
The second approach is difficult as you have to have solved the problem which your AI algorithm wants to approximate "in some other way".
This leads to a classic bootstrapping problem.
For example, if you would have an AI which is capable of deciding which data is important for the training of another AI: Why not use this AI to solve the original problem?

At this point it is still unclear to me which of the two approaches is the more promising one.

### Ideas for a solution

So, how can we tackle those problems?
The first two can actually be solved on a organizatorial level in terms of establishing a data mesh.
This, however, is hard to do as organizations grow over time and teams should not be broken up because new teams always need to find an efficient working mode.
We don't always have the means nor does it always make sense to re-shuffle teams.
A way to solve this on a technical level would be to introduce a well defined interface to the data which is right shifted as much as possible such that data scientists have to deal as little as possible with data pipelines.
A good interface seems to be an API where the data engineers/data platform team can "hide" their query engines behind.
This reduces the integration effort and enables cost reduction easily. 
However, you need an interface with enough throughput which is why `Apache Arrow Flight` becomes a valid option (you do not want to use REST).
This also helps with the curve flattening strategy for the third problem as a stable interface can be provided and query engines can be switched behind it.
Similarly, the query platform does not have to be a monolith and centralized. 

Tackling the second strategy in the third problem is even more challenging and it's viability largely depends on the type of AI problem you want to solve.
In some cases, you can incorporate human input to pick interesting data for training.
On large scales this is not viable anymore and the only hope is that there is a powerful foundation model which you can leverage.

### Final Thoughts 

Data pipelines are the backbone of modern analytics, yet their challenges often go unacknowledged. Balancing technical innovation with organizational alignment requires careful planning and adaptability. While no single solution fits all scenarios, a hybrid approach—combining clear ownership, decentralized tools, and intelligent cost management—offers a path forward.   

